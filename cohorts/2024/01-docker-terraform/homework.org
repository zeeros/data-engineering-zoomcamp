#+title: Module 1 Homework

* Docker & SQL
:PROPERTIES:
:header-args+: :eval never-export
:header-args+: :exports both
:header-args:sql+: :engine postgresql
:header-args:sql+: :dbhost localhost
:header-args:sql+: :dbuser root
:header-args:sql+: :dbpassword root
:header-args:sql+: :database ny_taxi
:header-args:sql+: :dbport 5432
:END:
** Knowing docker tags

Run the command to get information on Docker.

#+begin_src sh :results verbatim
docker --help
#+end_src

#+RESULTS:
#+begin_example

Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Common Commands:
  run         Create and run a new container from an image
  exec        Execute a command in a running container
  ps          List containers
  build       Build an image from a Dockerfile
  pull        Download an image from a registry
  push        Upload an image to a registry
  images      List images
  login       Log in to a registry
  logout      Log out from a registry
  search      Search Docker Hub for images
  version     Show the Docker version information
  info        Display system-wide information

Management Commands:
  builder     Manage builds
  buildx*     Docker Buildx (Docker Inc., v0.11.2)
  compose*    Docker Compose (Docker Inc., v2.21.0)
  container   Manage containers
  context     Manage contexts
  image       Manage images
  manifest    Manage Docker image manifests and manifest lists
  network     Manage networks
  plugin      Manage plugins
  system      Manage Docker
  trust       Manage trust on Docker images
  volume      Manage volumes

Swarm Commands:
  swarm       Manage Swarm

Commands:
  attach      Attach local standard input, output, and error streams to a running container
  commit      Create a new image from a container's changes
  cp          Copy files/folders between a container and the local filesystem
  create      Create a new container
  diff        Inspect changes to files or directories on a container's filesystem
  events      Get real time events from the server
  export      Export a container's filesystem as a tar archive
  history     Show the history of an image
  import      Import the contents from a tarball to create a filesystem image
  inspect     Return low-level information on Docker objects
  kill        Kill one or more running containers
  load        Load an image from a tar archive or STDIN
  logs        Fetch the logs of a container
  pause       Pause all processes within one or more containers
  port        List port mappings or a specific mapping for the container
  rename      Rename a container
  restart     Restart one or more containers
  rm          Remove one or more containers
  rmi         Remove one or more images
  save        Save one or more images to a tar archive (streamed to STDOUT by default)
  start       Start one or more stopped containers
  stats       Display a live stream of container(s) resource usage statistics
  stop        Stop one or more running containers
  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE
  top         Display the running processes of a container
  unpause     Unpause all processes within one or more containers
  update      Update configuration of one or more containers
  wait        Block until one or more containers stop, then print their exit codes

Global Options:
      --config string      Location of client config files (default
                           "/home/i2cat/.docker")
  -c, --context string     Name of the context to use to connect to the
                           daemon (overrides DOCKER_HOST env var and
                           default context set with "docker context use")
  -D, --debug              Enable debug mode
  -H, --host list          Daemon socket to connect to
  -l, --log-level string   Set the logging level ("debug", "info",
                           "warn", "error", "fatal") (default "info")
      --tls                Use TLS; implied by --tlsverify
      --tlscacert string   Trust certs signed only by this CA (default
                           "/home/i2cat/.docker/ca.pem")
      --tlscert string     Path to TLS certificate file (default
                           "/home/i2cat/.docker/cert.pem")
      --tlskey string      Path to TLS key file (default
                           "/home/i2cat/.docker/key.pem")
      --tlsverify          Use TLS and verify the remote
  -v, --version            Print version information and quit

Run 'docker COMMAND --help' for more information on a command.

For more help on how to use Docker, head to https://docs.docker.com/go/guides/
#+end_example

Now run the command to get help on the =docker build= command.

#+begin_src sh :results verbatim
docker build --help
#+end_src

#+RESULTS:
#+begin_example

Usage:  docker buildx build [OPTIONS] PATH | URL | -

Start a build

Aliases:
  docker buildx build, docker buildx b

Options:
      --add-host strings              Add a custom host-to-IP mapping
                                      (format: "host:ip")
      --allow strings                 Allow extra privileged entitlement
                                      (e.g., "network.host",
                                      "security.insecure")
      --attest stringArray            Attestation parameters (format:
                                      "type=sbom,generator=image")
      --build-arg stringArray         Set build-time variables
      --build-context stringArray     Additional build contexts (e.g.,
                                      name=path)
      --builder string                Override the configured builder
                                      instance (default "default")
      --cache-from stringArray        External cache sources (e.g.,
                                      "user/app:cache",
                                      "type=local,src=path/to/dir")
      --cache-to stringArray          Cache export destinations (e.g.,
                                      "user/app:cache",
                                      "type=local,dest=path/to/dir")
      --cgroup-parent string          Optional parent cgroup for the container
  -f, --file string                   Name of the Dockerfile (default:
                                      "PATH/Dockerfile")
      --iidfile string                Write the image ID to the file
      --label stringArray             Set metadata for an image
      --load                          Shorthand for "--output=type=docker"
      --metadata-file string          Write build result metadata to the file
      --network string                Set the networking mode for the
                                      "RUN" instructions during build
                                      (default "default")
      --no-cache                      Do not use cache when building the image
      --no-cache-filter stringArray   Do not cache specified stages
  -o, --output stringArray            Output destination (format:
                                      "type=local,dest=path")
      --platform stringArray          Set target platform for build
      --progress string               Set type of progress output
                                      ("auto", "plain", "tty"). Use plain
                                      to show container output (default
                                      "auto")
      --provenance string             Shorthand for "--attest=type=provenance"
      --pull                          Always attempt to pull all
                                      referenced images
      --push                          Shorthand for "--output=type=registry"
  -q, --quiet                         Suppress the build output and print
                                      image ID on success
      --sbom string                   Shorthand for "--attest=type=sbom"
      --secret stringArray            Secret to expose to the build
                                      (format:
                                      "id=mysecret[,src=/local/secret]")
      --shm-size bytes                Size of "/dev/shm"
      --ssh stringArray               SSH agent socket or keys to expose
                                      to the build (format:
                                      "default|<id>[=<socket>|<key>[,<key>]]")
  -t, --tag stringArray               Name and optionally a tag (format:
                                      "name:tag")
      --target string                 Set the target build stage to build
      --ulimit ulimit                 Ulimit options (default [])
#+end_example

Do the same for =docker run=.

#+begin_src sh :results verbatim
docker run --help
#+end_src

#+RESULTS:
#+begin_example

Usage:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...]

Create and run a new container from an image

Aliases:
  docker container run, docker run

Options:
      --add-host list                  Add a custom host-to-IP mapping
                                       (host:ip)
      --annotation map                 Add an annotation to the container
                                       (passed through to the OCI
                                       runtime) (default map[])
  -a, --attach list                    Attach to STDIN, STDOUT or STDERR
      --blkio-weight uint16            Block IO (relative weight),
                                       between 10 and 1000, or 0 to
                                       disable (default 0)
      --blkio-weight-device list       Block IO weight (relative device
                                       weight) (default [])
      --cap-add list                   Add Linux capabilities
      --cap-drop list                  Drop Linux capabilities
      --cgroup-parent string           Optional parent cgroup for the
                                       container
      --cgroupns string                Cgroup namespace to use
                                       (host|private)
                                       'host':    Run the container in
                                       the Docker host's cgroup namespace
                                       'private': Run the container in
                                       its own private cgroup namespace
                                       '':        Use the cgroup
                                       namespace as configured by the
                                                  default-cgroupns-mode
                                       option on the daemon (default)
      --cidfile string                 Write the container ID to the file
      --cpu-period int                 Limit CPU CFS (Completely Fair
                                       Scheduler) period
      --cpu-quota int                  Limit CPU CFS (Completely Fair
                                       Scheduler) quota
      --cpu-rt-period int              Limit CPU real-time period in
                                       microseconds
      --cpu-rt-runtime int             Limit CPU real-time runtime in
                                       microseconds
  -c, --cpu-shares int                 CPU shares (relative weight)
      --cpus decimal                   Number of CPUs
      --cpuset-cpus string             CPUs in which to allow execution
                                       (0-3, 0,1)
      --cpuset-mems string             MEMs in which to allow execution
                                       (0-3, 0,1)
  -d, --detach                         Run container in background and
                                       print container ID
      --detach-keys string             Override the key sequence for
                                       detaching a container
      --device list                    Add a host device to the container
      --device-cgroup-rule list        Add a rule to the cgroup allowed
                                       devices list
      --device-read-bps list           Limit read rate (bytes per second)
                                       from a device (default [])
      --device-read-iops list          Limit read rate (IO per second)
                                       from a device (default [])
      --device-write-bps list          Limit write rate (bytes per
                                       second) to a device (default [])
      --device-write-iops list         Limit write rate (IO per second)
                                       to a device (default [])
      --disable-content-trust          Skip image verification (default true)
      --dns list                       Set custom DNS servers
      --dns-option list                Set DNS options
      --dns-search list                Set custom DNS search domains
      --domainname string              Container NIS domain name
      --entrypoint string              Overwrite the default ENTRYPOINT
                                       of the image
  -e, --env list                       Set environment variables
      --env-file list                  Read in a file of environment variables
      --expose list                    Expose a port or a range of ports
      --gpus gpu-request               GPU devices to add to the
                                       container ('all' to pass all GPUs)
      --group-add list                 Add additional groups to join
      --health-cmd string              Command to run to check health
      --health-interval duration       Time between running the check
                                       (ms|s|m|h) (default 0s)
      --health-retries int             Consecutive failures needed to
                                       report unhealthy
      --health-start-period duration   Start period for the container to
                                       initialize before starting
                                       health-retries countdown
                                       (ms|s|m|h) (default 0s)
      --health-timeout duration        Maximum time to allow one check to
                                       run (ms|s|m|h) (default 0s)
      --help                           Print usage
  -h, --hostname string                Container host name
      --init                           Run an init inside the container
                                       that forwards signals and reaps
                                       processes
  -i, --interactive                    Keep STDIN open even if not attached
      --ip string                      IPv4 address (e.g., 172.30.100.104)
      --ip6 string                     IPv6 address (e.g., 2001:db8::33)
      --ipc string                     IPC mode to use
      --isolation string               Container isolation technology
      --kernel-memory bytes            Kernel memory limit
  -l, --label list                     Set meta data on a container
      --label-file list                Read in a line delimited file of labels
      --link list                      Add link to another container
      --link-local-ip list             Container IPv4/IPv6 link-local
                                       addresses
      --log-driver string              Logging driver for the container
      --log-opt list                   Log driver options
      --mac-address string             Container MAC address (e.g.,
                                       92:d0:c6:0a:29:33)
  -m, --memory bytes                   Memory limit
      --memory-reservation bytes       Memory soft limit
      --memory-swap bytes              Swap limit equal to memory plus
                                       swap: '-1' to enable unlimited swap
      --memory-swappiness int          Tune container memory swappiness
                                       (0 to 100) (default -1)
      --mount mount                    Attach a filesystem mount to the
                                       container
      --name string                    Assign a name to the container
      --network network                Connect a container to a network
      --network-alias list             Add network-scoped alias for the
                                       container
      --no-healthcheck                 Disable any container-specified
                                       HEALTHCHECK
      --oom-kill-disable               Disable OOM Killer
      --oom-score-adj int              Tune host's OOM preferences (-1000
                                       to 1000)
      --pid string                     PID namespace to use
      --pids-limit int                 Tune container pids limit (set -1
                                       for unlimited)
      --platform string                Set platform if server is
                                       multi-platform capable
      --privileged                     Give extended privileges to this
                                       container
  -p, --publish list                   Publish a container's port(s) to
                                       the host
  -P, --publish-all                    Publish all exposed ports to
                                       random ports
      --pull string                    Pull image before running
                                       ("always", "missing", "never")
                                       (default "missing")
  -q, --quiet                          Suppress the pull output
      --read-only                      Mount the container's root
                                       filesystem as read only
      --restart string                 Restart policy to apply when a
                                       container exits (default "no")
      --rm                             Automatically remove the container
                                       when it exits
      --runtime string                 Runtime to use for this container
      --security-opt list              Security Options
      --shm-size bytes                 Size of /dev/shm
      --sig-proxy                      Proxy received signals to the
                                       process (default true)
      --stop-signal string             Signal to stop the container
      --stop-timeout int               Timeout (in seconds) to stop a
                                       container
      --storage-opt list               Storage driver options for the
                                       container
      --sysctl map                     Sysctl options (default map[])
      --tmpfs list                     Mount a tmpfs directory
  -t, --tty                            Allocate a pseudo-TTY
      --ulimit ulimit                  Ulimit options (default [])
  -u, --user string                    Username or UID (format:
                                       <name|uid>[:<group|gid>])
      --userns string                  User namespace to use
      --uts string                     UTS namespace to use
  -v, --volume list                    Bind mount a volume
      --volume-driver string           Optional volume driver for the
                                       container
      --volumes-from list              Mount volumes from the specified
                                       container(s)
  -w, --workdir string                 Working directory inside the container
#+end_example

Which tag has the following text? - /Automatically remove the container when it exits/

#+begin_src sh :results verbatim
docker run --help | grep -A 1 "Automatically remove the container"
#+end_src

#+RESULTS:
:       --rm                             Automatically remove the container
:                                        when it exits

- [ ] =--delete=
- [ ] =--rc=
- [ ] =--rmc=
- [X] =--rm=

** Understanding docker first run

Run docker with the =python:3.9= image in an interactive mode and the entrypoint of bash.
Now check the python modules that are installed (use =pip list=).

#+begin_src sh :async
docker run --entrypoint /bin/bash python:3.9 -c "pip list" | grep "wheel"
#+end_src

#+RESULTS:
: wheel      0.42.0

What is version of the package =wheel=?

- [X] =0.42.0=
- [ ] =1.0.0=
- [ ] =23.0.1=
- [ ] =58.1.0=

** Prepare Postgres

Run Postgres and load data as shown in the videos.
We'll use the [[https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz][green taxi trips from September 2019]].
You will also need the [[https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv][dataset with zones]].
Download this data and put it into Postgres (with jupyter notebooks or with a pipeline).

#+begin_src python :dir ./ny_taxi_data :mkdirp yes :results output
import os
from urllib import request

def download_file(url, local_filename):
    with request.urlopen(url) as response:
        with open(local_filename, 'wb') as file:
            while True:
                chunk = response.read(1024)
                if not chunk:
                    break
                file.write(chunk)

def check_and_download(urls):
    for url in urls:
        filename = url.split("/")[-1]
        if not os.path.isfile(filename):
            print(f"Downloading {filename}...")
            download_file(url, filename)
            print(f"{filename} downloaded successfully.")
        else:
            print(f"{filename} already exists locally.")

if __name__ == "__main__":
    urls = [
        "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz",
        "https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
    ]

    check_and_download(urls)
#+end_src

#+RESULTS:
: Downloading green_tripdata_2019-09.csv.gz...
: green_tripdata_2019-09.csv.gz downloaded successfully.
: Downloading taxi+_zone_lookup.csv...
: taxi+_zone_lookup.csv downloaded successfully.

#+begin_src python :results output
import sys
import psycopg2
import sqlalchemy as sqla
from datetime import datetime
import pandas as pd

DB_HOST = 'localhost'
DB_PORT = '5432'
DB_NAME = 'ny_taxi'
DB_USER = 'root'
DB_PASSWORD = 'root'

engine = sqla.create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')

def load_csv_to_sql(csv_file_path, table_name, engine, parse_dates=None, chunksize=None):
    df = pd.read_csv(
        filepath_or_buffer=csv_file_path,
        parse_dates=parse_dates,
        nrows=1000
    )

    df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')

    csv_iter = pd.read_csv(
        filepath_or_buffer=csv_file_path,
        iterator=True,
        chunksize=chunksize,
        parse_dates=parse_dates
    )

    for i, chunk in enumerate(csv_iter):
        chunk.to_sql(name=table_name, con=engine, if_exists='append')

    print(f'Data written to table "{table_name}".')

load_csv_to_sql(
    csv_file_path='./ny_taxi_data/green_tripdata_2019-09.csv.gz',
    table_name='green_taxi_data',
    engine=engine,
    parse_dates=['lpep_pickup_datetime', 'lpep_dropoff_datetime'],
    chunksize=10**4
)
load_csv_to_sql(
    csv_file_path='./ny_taxi_data/taxi+_zone_lookup.csv',
    table_name='taxi_zone',
    engine=engine
)

engine.dispose()
#+end_src

#+RESULTS:
: Data written to table "green_taxi_data".
: Data written to table "taxi_zone".

#+begin_src sql
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_name = 'green_taxi_data'
#+end_src

#+RESULTS:
| column_name           | data_type                   |
|-----------------------+-----------------------------|
| congestion_surcharge  | double precision            |
| VendorID              | bigint                      |
| lpep_pickup_datetime  | timestamp without time zone |
| lpep_dropoff_datetime | timestamp without time zone |
| index                 | bigint                      |
| RatecodeID            | bigint                      |
| PULocationID          | bigint                      |
| DOLocationID          | bigint                      |
| passenger_count       | bigint                      |
| trip_distance         | double precision            |
| fare_amount           | double precision            |
| extra                 | double precision            |
| mta_tax               | double precision            |
| tip_amount            | double precision            |
| tolls_amount          | double precision            |
| ehail_fee             | double precision            |
| improvement_surcharge | double precision            |
| total_amount          | double precision            |
| payment_type          | bigint                      |
| trip_type             | bigint                      |
| store_and_fwd_flag    | text                        |

#+begin_src sql
SELECT COUNT(*) FROM green_taxi_data
#+end_src

#+RESULTS:
|  count |
|--------|
| 449063 |

#+begin_src sql
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_name = 'taxi_zone'
#+end_src

#+RESULTS:
| column_name  | data_type |
|--------------+-----------|
| index        | bigint    |
| LocationID   | bigint    |
| Borough      | text      |
| Zone         | text      |
| service_zone | text      |

#+begin_src sql
SELECT COUNT(*) FROM taxi_zone
#+end_src

#+RESULTS:
| count |
|-------|
|   265 |

** Count records

How many taxi trips were totally made on September 18th 2019?

Tip: started and finished on =2019-09-18=.

Remember that =lpep_pickup_datetime= and =lpep_dropoff_datetime= columns are in the format timestamp (=date= and =hour+min+sec=) and not in =date=.

#+begin_src sql
SELECT COUNT(*)
FROM green_taxi_data
WHERE DATE(lpep_pickup_datetime) = '2019-09-18'
  AND DATE(lpep_dropoff_datetime) = '2019-09-18'
#+end_src

#+RESULTS:
| count |
|-------|
| 15612 |

- [ ] =15767=
- [X] =15612=
- [ ] =15859=
- [ ] =89009=

** Largest trip for each day

Which was the pick up day with the largest trip distance?
Use the pick up time for your calculations.

#+begin_src sql
SELECT
  DATE_TRUNC('day', lpep_pickup_datetime) AS pickup_day,
  SUM(trip_distance) AS total_trip_distance
FROM green_taxi_data
GROUP BY pickup_day
ORDER BY total_trip_distance DESC
LIMIT 1;
#+end_src

#+RESULTS:
| pickup_day          | total_trip_distance |
|---------------------+---------------------|
| 2019-09-26 00:00:00 |    58759.9400000002 |

- [ ] =2019-09-18=
- [ ] =2019-09-16=
- [X] =2019-09-26=
- [ ] =2019-09-21=

** Three biggest pick up Boroughs

Consider =lpep_pickup_datetime= in =2019-09-18= and ignore =Borough= has =Unknown=.
Which were the 3 pick up Boroughs that had a sum of =total_amount= superior to 50000?

#+begin_src sql
SELECT "Borough", SUM(total_amount) AS total_amount_sum
FROM green_taxi_data LEFT JOIN taxi_zone
  ON "PULocationID" = "LocationID"
WHERE DATE(lpep_pickup_datetime) = '2019-09-18'
GROUP BY "Borough"
HAVING SUM(total_amount) > 50000
ORDER BY total_amount_sum DESC
#+end_src

#+RESULTS:
| Borough   |  total_amount_sum |
|-----------+-------------------|
| Brooklyn  | 96333.23999999902 |
| Manhattan | 92271.29999999839 |
| Queens    | 78671.70999999889 |

- [X] =Brooklyn, Manhattan, Queens=
- [ ] =Bronx, Brooklyn, Manhattan=
- [ ] =Bronx, Manhattan, Queens=
- [ ] =Brooklyn, Queens, Staten Island=

** Largest tip

For the passengers picked up in September 2019 in the zone name =Astoria= which was the drop off zone that had the largest tip?
We want the name of the zone, not the id.

Note: it's not a typo, it's =tip= , not =trip=

#+begin_src sql
SELECT
    tz_dropoff."Zone" AS dropoff_zone_name,
    MAX(gt.tip_amount) AS max_tip_amount
FROM
    green_taxi_data gt
JOIN
    taxi_zone tz_pickup ON gt."PULocationID" = tz_pickup."LocationID"
JOIN
    taxi_zone tz_dropoff ON gt."DOLocationID" = tz_dropoff."LocationID"
WHERE
    tz_pickup."Zone" = 'Astoria'
    AND EXTRACT(MONTH FROM gt."lpep_pickup_datetime") = 9
    AND EXTRACT(YEAR FROM gt."lpep_pickup_datetime") = 2019
GROUP BY
    tz_dropoff."Zone"
ORDER BY
    max_tip_amount DESC
LIMIT 1;
#+end_src

#+RESULTS:
| dropoff_zone_name | max_tip_amount |
|-------------------+----------------|
| JFK Airport       |          62.31 |

- [ ] =Central Park=
- [ ] =Jamaica=
- [X] =JFK Airport=
- [ ] =Long Island City/Queens Plaza=

* Terraform

In this section homework we'll prepare the environment by creating resources in GCP with Terraform.

In your VM on GCP/Laptop/GitHub Codespace install Terraform.
Copy the files from the course repo [[https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/01-docker-terraform/1_terraform_gcp/terraform][here]] to your VM/Laptop/GitHub Codespace.

Modify the files as necessary to create a GCP Bucket and Big Query Dataset.

1. Login to GCP
2. Create a new project and name it =terraform-module-1=
3. Go to =IAM & Admin > Service Accounts > CREATE SERVICE ACCOUNT=
   1) Name the service account =terraform-runner=
   2) Add the roles =Storage Admin=, =BigQuery Admin= and =Compute Admin=
4. In the page =Service accounts=, go to =Manage keys= under the new service account
   1) Click =ADD KEY > Create a new key > JSON=
5. Go to =BigQuery= and ensure it is enabled

** Creating Resources

#+begin_src sh :tangle main.tf
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "5.13.0"
    }
  }
}

provider "google" {
  credentials = file(var.credentials)
  project     = var.project
  region      = var.region
}

resource "google_storage_bucket" "demo-bucket" {
  name          = var.gcs_bucket_name
  location      = var.location
  force_destroy = true
  lifecycle_rule {
    condition {
      age = 1
    }
    action {
      type = "AbortIncompleteMultipartUpload"
    }
  }
}

resource "google_bigquery_dataset" "demo_dataset" {
  dataset_id = var.bq_dataset_name
  location   = var.location
}
#+end_src

#+begin_src sh :tangle variables.tf
variable "credentials" {
  default     = "~/src/data-engineering-zoomcamp/module-1/01-docker-terraform/1_terraform_gcp/terraform/credentials.json"
}

variable "project" {
  default     = "terraform-module-1"
}

variable "region" {
  default     = "europe-southwest1"
}

variable "location" {
  default     = "EU"
}

variable "bq_dataset_name" {
  default     = "demo_dataset"
}

variable "gcs_bucket_name" {
  default     = "terraform-demo-terra-bucket-safgasfwae"
}

variable "gcs_storage_class" {
  default     = "STANDARD"
}
#+end_src

After updating the ~main.tf~ and ~variable.tf~ files run:

#+begin_src sh :results verbatim
terraform apply -auto-approve -no-color
#+end_src

#+RESULTS:
#+begin_example

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # google_bigquery_dataset.demo_dataset will be created
  + resource "google_bigquery_dataset" "demo_dataset" {
      + creation_time              = (known after apply)
      + dataset_id                 = "demo_dataset"
      + default_collation          = (known after apply)
      + delete_contents_on_destroy = false
      + effective_labels           = (known after apply)
      + etag                       = (known after apply)
      + id                         = (known after apply)
      + is_case_insensitive        = (known after apply)
      + last_modified_time         = (known after apply)
      + location                   = "EU"
      + max_time_travel_hours      = (known after apply)
      + project                    = "terraform-module-1"
      + self_link                  = (known after apply)
      + storage_billing_model      = (known after apply)
      + terraform_labels           = (known after apply)
    }

  # google_storage_bucket.demo-bucket will be created
  + resource "google_storage_bucket" "demo-bucket" {
      + effective_labels            = (known after apply)
      + force_destroy               = true
      + id                          = (known after apply)
      + location                    = "EU"
      + name                        = "terraform-demo-terra-bucket-safgasfwae"
      + project                     = (known after apply)
      + public_access_prevention    = (known after apply)
      + rpo                         = (known after apply)
      + self_link                   = (known after apply)
      + storage_class               = "STANDARD"
      + terraform_labels            = (known after apply)
      + uniform_bucket_level_access = (known after apply)
      + url                         = (known after apply)

      + lifecycle_rule {
          + action {
              + type = "AbortIncompleteMultipartUpload"
            }
          + condition {
              + age                   = 1
              + matches_prefix        = []
              + matches_storage_class = []
              + matches_suffix        = []
              + with_state            = (known after apply)
            }
        }
    }

Plan: 2 to add, 0 to change, 0 to destroy.
google_bigquery_dataset.demo_dataset: Creating...
google_storage_bucket.demo-bucket: Creating...
google_bigquery_dataset.demo_dataset: Creation complete after 2s [id=projects/terraform-module-1/datasets/demo_dataset]
google_storage_bucket.demo-bucket: Creation complete after 2s [id=terraform-demo-terra-bucket-safgasfwae]

Apply complete! Resources: 2 added, 0 changed, 0 destroyed.
#+end_example

Paste the output of this command into the homework submission form.

** TODO Submitting the solutions

- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw01
- You can submit your homework multiple times. In this case, only the last submission will be used.

Deadline: 29 January, 23:00 CET

* Docker setup

#+begin_src sh :session v :noweb yes :noweb-prefix no :results none
echo '<<docker-compose-yml>>' | docker compose -f - up --detach &
#+end_src

#+begin_src sh :session v :noweb yes :noweb-prefix no :results none
echo '<<docker-compose-yml>>' | docker compose -f - down &
#+end_src

#+name: python-dockerfile
#+begin_src sh
FROM python:3.9.1
RUN apt-get install wget
RUN pip install pandas sqlalchemy psycopg2
WORKDIR /data
#+end_src

#+name: docker-compose-yml
#+begin_src sh :noweb yes
name: module-1
services:
  db:
    image: postgres:16.1
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=ny_taxi
    volumes:
      - "./ny_taxi_data/postgres:/var/lib/postgresql/data:rw"
    ports:
      - "5432:5432"
  datapipeline:
    build:
      context: .
      dockerfile_inline: |
        <<python-dockerfile>>
    depends_on: [db]
    entrypoint: ["tail", "-f", "/dev/null"]
    volumes:
      - "./ny_taxi_data:/data:rw"
#+end_src

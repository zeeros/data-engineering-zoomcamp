{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c701dd-9b28-478b-a7e8-5770a8b7705d",
   "metadata": {},
   "source": [
    "## Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df775ab4-23d9-49c4-969b-20075f5cb62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840ebed2-dcc2-42f9-8497-d55503ee1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b1f98-85fe-4cd0-be8e-edb70df43c34",
   "metadata": {},
   "source": [
    "## FHV October 2019 partition size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b03eb23e-c83c-43a4-b066-a806bbdfff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType(spark.createDataFrame(pd.read_csv('fhv_tripdata_2019-10.csv', nrows=1000)).schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d94f03-650c-454e-b6be-e96a0f578e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57870d87-0f46-4807-9857-3f36c454c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)\n",
    "df.write.parquet('partitions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b895858c-47ae-4387-8e15-9b367cc36fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the file 'part-00005-f24464ca-7ae1-4f6a-b438-f79644d8edc3-c000.snappy.parquet' is 5.66 MB\n",
      "The size of the file 'part-00000-f24464ca-7ae1-4f6a-b438-f79644d8edc3-c000.snappy.parquet' is 5.66 MB\n",
      "The size of the file 'part-00002-f24464ca-7ae1-4f6a-b438-f79644d8edc3-c000.snappy.parquet' is 5.66 MB\n",
      "The size of the file 'part-00003-f24464ca-7ae1-4f6a-b438-f79644d8edc3-c000.snappy.parquet' is 5.65 MB\n",
      "The size of the file 'part-00004-f24464ca-7ae1-4f6a-b438-f79644d8edc3-c000.snappy.parquet' is 5.67 MB\n",
      "The size of the file 'part-00001-f24464ca-7ae1-4f6a-b438-f79644d8edc3-c000.snappy.parquet' is 5.66 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = 'partitions/'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if os.path.isfile(file_path) and filename.endswith('.parquet'):\n",
    "        file_size_bytes = os.path.getsize(file_path)\n",
    "        file_size_mb = file_size_bytes / 1024 ** 2\n",
    "        print(f\"The size of the file '{os.path.basename(file_path)}' is {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9c303-f3b6-4837-98f4-00485aa4e0f9",
   "metadata": {},
   "source": [
    "## Count records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac4bb8a2-d793-4e1f-8980-c3aafbe1566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8777b26-1219-4a43-828f-b8c68449e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"pickup_date\", col(\"pickup_datetime\").cast(\"date\"))\n",
    "october_15_trips = df.filter(col(\"pickup_date\") == \"2019-10-15\")\n",
    "october_15_trips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64500e59-dbc5-43b9-9c61-47e65723ee7c",
   "metadata": {},
   "source": [
    "## Longest trip for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f6973e6-65d1-4f64-adb5-e47a173f37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eab5d111-6cb3-47d7-993b-72a1776a4ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest trip in hours: 631152.5\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"pickup_timestamp\", unix_timestamp(col(\"pickup_datetime\")))\n",
    "df = df.withColumn(\"dropOff_timestamp\", unix_timestamp(col(\"dropOff_datetime\")))\n",
    "df = df.withColumn(\"trip_duration\", col(\"dropOff_timestamp\") - col(\"pickup_timestamp\"))\n",
    "max_duration_seconds = df.agg({\"trip_duration\": \"max\"}).collect()[0][0]\n",
    "max_duration_hours = max_duration_seconds / 3600\n",
    "print(f\"Length of the longest trip in hours: {max_duration_hours}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362b1da-ea6b-433d-85bd-af6efef46a32",
   "metadata": {},
   "source": [
    "## Least frequent pickup location zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c18684b-0cd8-4f17-b289-fa34001a811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent pickup location zone: 2.0\n"
     ]
    }
   ],
   "source": [
    "pickup_counts = df.groupBy(\"PUlocationID\").count()\n",
    "least_frequent_pickup_zone = pickup_counts.orderBy(col(\"count\")).limit(1).select(\"PUlocationID\").collect()[0][0]\n",
    "print(f\"Least frequent pickup location zone: {least_frequent_pickup_zone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf1ae19e-797d-4c02-a091-76f30e2b5125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_lookup_df = spark.read.csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "zone_lookup_df.createOrReplaceTempView(\"taxi_zone_lookup\")\n",
    "zone_lookup_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
